{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9eceb907",
   "metadata": {},
   "source": [
    "# Part 2: Pneumonia Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed1a5b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from os ipipmport listdir\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11522dc3",
   "metadata": {},
   "source": [
    "### Q2 CNN Classifier (3 Pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbd3876",
   "metadata": {},
   "source": [
    "Data Preprocessing using histogram equalization and gaussian blurring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01da81ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "\n",
    "    # Resize the image\n",
    "    img = cv2.resize(img, (1000, 1000))\n",
    "\n",
    "    # Convert to grayscale\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Histogram Equalization\n",
    "    img = cv2.equalizeHist(img)\n",
    "\n",
    "    # Gaussian Blur (5x5 kernel)\n",
    "    final_img = cv2.GaussianBlur(img, (5, 5), 0)\n",
    "    \n",
    "    return final_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dbb0af",
   "metadata": {},
   "source": [
    "Comparison between original image and processed image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bea5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_normal1 = np.asarray(Image.open('data_part2/train/NORMAL/IM-0115-0001.jpeg'))\n",
    "\n",
    "img_processed = preprocess_image('data_part2/train/NORMAL/IM-0115-0001.jpeg')\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 7))\n",
    "\n",
    "axes[0].imshow(img_normal1, cmap='gray')\n",
    "axes[0].set_title('Original Image')\n",
    "\n",
    "axes[1].imshow(img_processed, cmap='gray')\n",
    "axes[1].set_title('Processed Image')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad38a001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing preprocessing on all images for training set, validation set, and test set\n",
    "\n",
    "# Need to do this for the following pairs {train, val, test} x {NORMAL, PNEUMONIA} (already done, see data_part2_processed folder)\n",
    "folder_dir = 'data_part2/test/PNEUMONIA/'\n",
    "processed_dir = 'data_part2_processed/test/PNEUMONIA/'\n",
    "\n",
    "for images in os.listdir(folder_dir):\n",
    "    processed_image = preprocess_image(os.path.join(folder_dir, images))\n",
    "\n",
    "    cv2.imwrite(os.path.join(processed_dir, images), processed_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231522b4",
   "metadata": {},
   "source": [
    "#### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18ffb345",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "trainset = ImageFolder(root=\"data_part2_processed/train/\", transform=transform)\n",
    "trainloader = DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "\n",
    "validationset = ImageFolder(root=\"data_part2_processed/val/\", transform=transform)\n",
    "validationloader = DataLoader(validationset, batch_size=32, shuffle=True)\n",
    "\n",
    "testset = ImageFolder(root=\"data_part2_processed/test/\", transform=transform)\n",
    "testloader = DataLoader(testset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "521e89d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 1000, 1000])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "images, labels = next(iter(trainloader))\n",
    "print(images.shape)  # should be [32, 1, 1000, 1000]\n",
    "print(labels.shape)  # should be [32]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cc87e8",
   "metadata": {},
   "source": [
    "#### CNN and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be9b0779",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=5, stride=2, padding=2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2, padding=2)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=5, stride=2, padding=2)\n",
    "        self.conv4 = nn.Conv2d(64, 128, kernel_size=5, stride=2, padding=2)\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a37e347",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNN().to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "802f2fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [01:18<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Training Loss: 0.5647, Validation Loss: 0.7599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [01:13<00:00,  2.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Training Loss: 0.5212, Validation Loss: 0.6750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [01:13<00:00,  2.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Training Loss: 0.5102, Validation Loss: 0.6171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [01:13<00:00,  2.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Training Loss: 0.4793, Validation Loss: 0.5749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [01:13<00:00,  2.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Training Loss: 0.3428, Validation Loss: 0.7552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [01:13<00:00,  2.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Training Loss: 0.2596, Validation Loss: 0.7356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [01:13<00:00,  2.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Training Loss: 0.2271, Validation Loss: 0.5615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [01:13<00:00,  2.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Training Loss: 0.2265, Validation Loss: 0.7079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [01:13<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Training Loss: 0.1901, Validation Loss: 0.6515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [01:13<00:00,  2.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Training Loss: 0.1851, Validation Loss: 0.5199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    model.train()\n",
    "\n",
    "    for images, labels in tqdm(trainloader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels.float().unsqueeze(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(trainloader)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in validationloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels.float().unsqueeze(1))\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    avg_val_loss = val_loss / len(validationloader)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf50d350",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:08<00:00,  2.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.1593, Test Accuracy: 72.60%\n",
      "Test AUROC: 0.7892, Test AUPRC: 0.8043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluation on test set\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(testloader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels.float().unsqueeze(1))\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # Accuracy calculation\n",
    "        predicted = (outputs > 0.5).float()\n",
    "        correct += (predicted == labels.unsqueeze(1)).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        # AUROC and AUPRC calculation\n",
    "        all_preds.extend(outputs.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "avg_test_loss = test_loss / len(testloader)\n",
    "accuracy = correct / total\n",
    "auroc = roc_auc_score(all_labels, all_preds)\n",
    "auprc = average_precision_score(all_labels, all_preds)\n",
    "\n",
    "print(f\"Test Loss: {avg_test_loss:.4f}, Test Accuracy: {accuracy*100:.2f}%\")\n",
    "print(f\"Test AUROC: {auroc:.4f}, Test AUPRC: {auprc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "956a5232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model weights\n",
    "torch.save(model.state_dict(), 'part2_cnn_weights/CNN_weights.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
