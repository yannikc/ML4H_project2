{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4 NAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('data/processed_data/X_train_processed.csv')\n",
    "y_train = pd.read_csv('data/processed_data/y_train_processed.csv')['HeartDisease']\n",
    "X_test = pd.read_csv('data/processed_data/X_test_processed.csv')\n",
    "y_test = pd.read_csv('data/processed_data/y_test.csv')['HeartDisease']\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Testing data shape: {X_test.shape}\")\n",
    "\n",
    "# Check feature names to understand the data\n",
    "print(\"\\nFeatures in the dataset:\")\n",
    "print(X_train.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "\n",
    "# 1) Define a Dataset that wraps X (DataFrame) and y (Series)\n",
    "class HeartDiseaseDataset(Dataset):\n",
    "    def __init__(self, X: pd.DataFrame, y: pd.Series, regression: bool):\n",
    "        # convert to contiguous numpy arrays then tensors\n",
    "        self.X = torch.tensor(X.values, dtype=torch.float32)\n",
    "        dtype_target = torch.float32 if regression else torch.long\n",
    "        self.y = torch.tensor(y.values, dtype=dtype_target)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "def build_dataloaders(config):\n",
    "    # 2) Load your processed CSVs\n",
    "    X = pd.read_csv('data/processed_data/X_train_processed.csv')\n",
    "    y = pd.read_csv('data/processed_data/y_train_processed.csv')['HeartDisease']\n",
    "\n",
    "    # 3) Prepare K-Fold splits\n",
    "    kf = KFold(\n",
    "        n_splits=config.n_folds,\n",
    "        shuffle=True,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    dataloaders = []\n",
    "    for train_idx, val_idx in kf.split(X):\n",
    "        X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "        train_ds = HeartDiseaseDataset(X_tr, y_tr, regression=config.regression)\n",
    "        val_ds   = HeartDiseaseDataset(X_val, y_val, regression=config.regression)\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_ds,\n",
    "            batch_size=config.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=config.num_workers\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            val_ds,\n",
    "            batch_size=config.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=config.num_workers\n",
    "        )\n",
    "        dataloaders.append((train_loader, val_loader))\n",
    "\n",
    "    # 4) Optionally build a test_loader\n",
    "    X_t = pd.read_csv('data/processed_data/X_test_processed.csv')\n",
    "    y_t = pd.read_csv('data/processed_data/y_test.csv')['HeartDisease']\n",
    "    test_ds = HeartDiseaseDataset(X_t, y_t, regression=config.regression)\n",
    "    test_loader = DataLoader(\n",
    "        test_ds,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=config.num_workers\n",
    "    )\n",
    "\n",
    "    return dataloaders, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from nam_trainer_pytorch import NAMTrainer  # wherever you put it\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dataloaders, test_loader = build_dataloaders(config)\n",
    "\n",
    "for fold, (train_loader, val_loader) in enumerate(dataloaders):\n",
    "    # 1. set up TensorBoard writer\n",
    "    log_dir = os.path.join(\n",
    "        config.logdir,\n",
    "        model.name,\n",
    "        f'fold_{fold + 1}'\n",
    "    )\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    # 2. initialize our pure-PyTorch trainer\n",
    "    trainer = NAMTrainer(config, model, device=device)\n",
    "\n",
    "    # 3. keep track of top-k checkpoints by val loss\n",
    "    best_ckpts = []  # list of (val_loss, checkpoint_path)\n",
    "\n",
    "    # 4. training loop\n",
    "    for epoch in range(1, config.num_epochs + 1):\n",
    "        print(f\"=== Fold {fold + 1}, Epoch {epoch}/{config.num_epochs} ===\")\n",
    "        train_loss, train_metric = trainer.train_epoch(train_loader)\n",
    "        val_loss,   val_metric   = trainer.validate_epoch(val_loader)\n",
    "\n",
    "        # 5. log scalars to TensorBoard\n",
    "        writer.add_scalars('Loss', {\n",
    "            'train': train_loss,\n",
    "            'val':   val_loss\n",
    "        }, epoch)\n",
    "        writer.add_scalars(trainer.metric_name, {\n",
    "            'train': train_metric,\n",
    "            'val':   val_metric\n",
    "        }, epoch)\n",
    "\n",
    "        # 6. checkpoint every epoch, keep best `save_top_k`\n",
    "        ckpt_filename = f'epoch_{epoch:02d}-val_{val_loss:.4f}.pt'\n",
    "        ckpt_path     = os.path.join(log_dir, ckpt_filename)\n",
    "        torch.save({\n",
    "            'epoch':                  epoch,\n",
    "            'model_state_dict':       model.state_dict(),\n",
    "            'optimizer_state_dict':   trainer.optimizer.state_dict()\n",
    "        }, ckpt_path)\n",
    "\n",
    "        # track & prune\n",
    "        best_ckpts.append((val_loss, ckpt_path))\n",
    "        best_ckpts = sorted(best_ckpts, key=lambda x: x[0])\n",
    "        if len(best_ckpts) > config.save_top_k:\n",
    "            # remove worst\n",
    "            worst_loss, worst_path = best_ckpts.pop(-1)\n",
    "            if os.path.exists(worst_path):\n",
    "                os.remove(worst_path)\n",
    "\n",
    "    writer.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
